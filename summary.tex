\documentclass{article}[12pt]


\usepackage{amsfonts,amsmath,amscd,amssymb,amsthm}
\usepackage{mathtools}
\usepackage{graphicx}
\usepackage{geometry}
\usepackage{geometry}
\usepackage{setspace}
\usepackage{hyperref}
\usepackage{relsize}

\def\lc{\left\lceil}   
\def\rc{\right\rceil}
\newtheorem{assumption}{Assumption}
\newtheorem{assumptionalt}{Assumption}[assumption]
\newenvironment{assumptionp}[1]{
  \renewcommand\theassumptionalt{#1}
  \assumptionalt
}{\endassumptionalt}


\geometry{ margin=1in}
\doublespacing


\title{\textbf{Summary on basic time series studies\\ \large tensor data analysis with different data types}}
\author{Haofan Zheng}
\date{}

\begin{document}


\maketitle
\newpage
\tableofcontents
\newpage

\section{High-dimentional $\alpha$-PCA method}
\subsection{Overall Summary}
\noindent This article considers the estimation and inference of the \textbf{low rank} components in high-dimentional matrixvariate models(tensor), and we propose an estimation method called $\alpha$-PCA and it has some benefits with the high dimensions data favorably compared with other methods(traditional PCA, etc) based on the perfomance in the simulation.



\subsection{Main model}\label{section main model 1.2}
\noindent The model is shown as the following:

\begin{equation}
    \mathbf{Y}_t = \underbrace{\mathbf{R}\mathbf{F}_t\mathbf{C}^T}_\text{signal part}+ \underbrace{\mathbf{E}_t}_\text{noise part}\
    \label{expression main model 1.2}
\end{equation}


$\mathbf{Y}_t: \mathbf{Y}_t\in \mathbb{R}^{p\times q}$, $1\leq t \leq T$, observations,

$\mathbf{F}_t: \mathbf{F}_t\in \mathbb{R}^{k\times r}$, where $k\ll p$ and $r\ll q$ (\textbf{low rank}), latent matrix,

$\mathbf{E}_t: \mathbf{E}_t\in \mathbb{R}^{p \times q}$, noise matrix.

\subsection{Main Statistics}\label{main statistics 1.3}
\noindent An estimation procedure, namely $\alpha$-PCA, aggregates the information in both first and second moments. Specifically, the two statistics are defined:
\begin{equation}
    \mathbf{\widehat{M}}_R  \overset{\Delta}{=} \dfrac{1}{pq}\Bigg((1 + \alpha) \cdot \mathbf{\overline{Y}}\mathbf{\overline{Y}}^T + \dfrac{1}{T}\sum\limits_{t=1}^T(\mathbf{Y}_t - \mathbf{\overline{Y}})(\mathbf{Y}_t - \mathbf{\overline{Y}})^T\Bigg)
    \label{statistics MR 1.3}
\end{equation}

\begin{equation}
    \mathbf{\widehat{M}}_C  \overset{\Delta}{=} \dfrac{1}{pq}\Bigg((1 + \alpha) \cdot  \mathbf{\overline{Y}}^T\mathbf{\overline{Y}} + \dfrac{1}{T}\sum\limits_{t=1}^T(\mathbf{Y}_t - \mathbf{\overline{Y}})^T(\mathbf{Y}_t - \mathbf{\overline{Y}})\Bigg)
    \label{statistics MC 1.3}
\end{equation}


$\alpha: \alpha \in \left[-1, + \infty \right)$, a hyperparameter,

$\mathbf{\overline{Y}} = \dfrac{1}{T} \sum\limits_{i=1}^T\mathbf{Y}_t$, the sample mean.

\noindent Based on these two statistics, estimation of $\mathbf{R}$ and $\mathbf{C}$ can be obtained as $\sqrt{p}$ times the top $k$ eigenvectors of $\widehat{\mathbf{M}}_R$ and $\sqrt{q}$ times the top $q$ eigenvectors of $\widehat{\mathbf{M}}_C$ respectively, in descending order by corresponding eigenvalues.

\subsection{Transformation}\label{transformation 1.4}
\noindent To simplify the estimator, we can transform the parameters, let the $\tilde{\alpha} = \sqrt{\alpha + 1} - 1$, $\mathbf{\widetilde{Y}}_t \overset{\Delta}{=} \mathbf{Y}_t + \tilde{\alpha}\mathbf{\overline{Y}}$, $\mathbf{\widetilde{F}}_t \overset{\Delta}{=} \mathbf{F}_t + \tilde{\alpha}\mathbf{\overline{F}}$, and $\mathbf{\widetilde{E}}_t \overset{\Delta}{=} \mathbf{E}_t + \tilde{\alpha}\mathbf{\overline{E}}$, Then we have

\begin{equation}
    \mathbf{\widetilde{Y}}_t = \mathbf{R}\mathbf{\widetilde{F}}_t\mathbf{C}^T + \mathbf{\widetilde{E}}_t 
\end{equation}

\noindent The equation \ref{statistics MR 1.3} and \ref{statistics MC 1.3} can be rewritten as:

\begin{equation}
    \mathbf{\widehat{M}}_R = \frac{1}{pqT}\sum\limits_{t=1}^{T}\mathbf{\widetilde{Y}}_t\mathbf{\widetilde{Y}}_t^T \text{, and } \mathbf{\widehat{M}}_C = \frac{1}{pqT}\sum\limits_{t=1}^{T}\mathbf{\widetilde{Y}}_t^T\mathbf{\widetilde{Y}}_t
\end{equation}

\noindent Same as in section \ref{main statistics 1.3}, $\mathbf{\widehat{R}}$ and $\mathbf{\widehat{C}}$ can be obtained as $\sqrt{p}$ times the top $k$ eigenvectors of $\widehat{\mathbf{M}}_R$ and $\sqrt{q}$ times the top $q$ eigenvectors of $\widehat{\mathbf{M}}_C$ respectively, in descending order by corresponding eigenvalues.

\subsection{Interpretation}

\noindent The estimator in Section \ref{section main model 1.2} approximately minimized jointly the unexplained variation and bias
\begin{equation}
    \begin{aligned}
    & \underset{\mathbf{R},\mathbf{C},\{\mathbf{F}_t\}_{t=1}^T}{\textbf{minimize}}
    & & (1+\alpha)\underbrace{\dfrac{1}{pq}\left\lVert \mathbf{\overline{Y}} - \mathbf{R}\mathbf{\overline{F}}\mathbf{C}^T\right\rVert_{F}^2}_\text{sample bias} + \underbrace{\dfrac{1}{pqT}\sum\limits_{t=1}^{T}\left\lVert \mathbf{Y}_t - \mathbf{R}\mathbf{F}\mathbf{C}^T\right\rVert_{F}^2}_\text{sample variance}\\
    & \textbf{subject to}
    & & \frac{1}{p}\mathbf{R}^T\mathbf{R} = \mathbf{I} \text{ , } \frac{1}{q}\mathbf{C}^T\mathbf{C} = \mathbf{I}
    \end{aligned}
\end{equation}

\noindent The special case for $\alpha = -1$ corresponds to the least-square estimator.(\textbf{\textit{not convex}})

\noindent Projecting on $\mathbf{R}$:

\begin{equation}
    \begin{aligned}
    & \underset{\mathbb{R}}{\textbf{maximize}}
    & & Tr\Bigg(\mathbb{E}\Bigg[(1+\alpha)(\mathbf{R}^T\mathbf{\overline{Y}})(\mathbf{R}^T\mathbf{\overline{Y}})^T+(\mathbf{R}^T\mathbf{Y}_t-\mathbb{E}\left[{R}^T\mathbf{Y}_t\right])(\mathbf{R}^T\mathbf{Y}_t-\mathbb{E}\left[{R}^T\mathbf{Y}_t\right])^T\Bigg]\Bigg)\\
    & \textbf{subject to}
    & & \frac{1}{p}\mathbf{R}^T\mathbf{R} = \mathbf{I}, \frac{1}{q}\mathbf{C}^T\mathbf{C} = \mathbf{I}
    \end{aligned}
\end{equation}

\noindent Where
$\mathbf{M}_R \overset{\Delta}{=} (1+\alpha)\mathbf{M}_R^{(1)}+\mathbf{M}_R^{(2)}, \mathbf{M}_R^{(1)} \overset{\Delta}{=} \dfrac{1}{pq}\mathbb{E}\left[\mathbf{\overline{Y}}\mathbf{\overline{Y}}^T\right] \text{, and } \mathbf{M}_R^{(2)} \overset{\Delta}{=} \dfrac{1}{pq}\mathbb{E}\left[\left(\mathbf{Y}_t-\mathbb{\left[\mathbf{\overline{Y}}\right]}\right)\left(\mathbf{Y}_t-\mathbb{\left[\mathbf{\overline{Y}}\right]}\right)^T\right]$

\noindent Then a solution by maximizing row and column variances respectively after projection is considered, projecting on $\mathbf{C}$ is similar.(\textbf{\textit{convex}})


\subsection{Relative estimators}
Based on the section \ref{transformation 1.4}, 
$$\mathbf{\widehat{F}}_t = \dfrac{1}{pq}\mathbf{\widehat{R}}^T\mathbf{\widehat{Y}}_t\mathbf{\widehat{C}}\text{, and the signal part }\mathbf{\widehat{S}}_t = \dfrac{1}{pq}\mathbf{\widehat{R}}\mathbf{\widehat{R}}^T\mathbf{\widehat{Y}}_t\mathbf{\widehat{C}}\mathbf{\widehat{C}}^T$$
 
\noindent Dimensions $k$ and $r$ are need to be determined:
\begin{enumerate}
    \item the eigenvalue ratio-based estimator, proposed by Ahn and Horestein(2013) 
    \item the Scree plot which is standard in principal component analysis. 
\end{enumerate}
\noindent Let $\hat{\lambda}_1 \geq \hat{\lambda} \geq \cdots \geq \hat{\lambda}_k \geq 0$ be the ordered eigenvalues of $\mathbf{\widehat{M}}_R$. The ratio-based estimator for $k$ is defined as follows:
    $$\widehat{k} = \underset{1 \leq j \leq k_{max}}{\arg\max }\dfrac{\widehat{\lambda}_j}{\widehat{\lambda}_{j+1}}$$
    where $k_{max}$ is the upper bound, usually taken as $\lc\dfrac{p}{2}\rc$ or $\lc\dfrac{p}{3}\rc$, according to Ahn and Horestein(2013),  similarly for $\widehat{r}$ with respect to $\mathbf{\widehat{M}}_C$.  
\subsection{Theoretical Properties}
\subsubsection{Assumptions}
\begin{assumption}\label{assumption 1 1.7.1}
    $\alpha$-\textbf{mixing}. \normalfont The vectorized factor VEC$(\mathbf{F}_t)$ and noise VEC$(\mathbf{E}_t)$ are $\alpha$-mixing. Specifically, a vector process $\{\mathbf{x}_t, t=0, \pm 1,\pm 2,\cdots\}$ is $\alpha$-mixing if, for some $\gamma \geq 2$, the mixing coefficients satisfy the condition that
    $$\mathlarger{\sum}_{h=1}^{+\infty} \alpha(h)^{1-\frac{2}{\gamma}} < \infty$$
    where $\alpha(h) =  \adjustlimits\sup_{\tau}\sup_{A \in \mathcal{F}_{-\infty}^{\tau}, B \in \mathcal{F}_{\tau + h}^{\infty}}|P(A\cap B)-P(A)P(B)|$ and $\mathcal{F}_{\tau}^s$ is the $\sigma$-field generated by $\{\mathbf{x}_t:\tau \leq t \leq s\}$.(\textbf{\textit{only deal with temporal dependence}})
\end{assumption}
 
\begin{assumption}\label{assumption 2 1.7.1}
    Factor and noise matrices. \normalfont There exists a positive constant $C < \infty$ such that for all $N$ and $T$,
    \begin{enumerate}
        \item Factor matrix $\mathbf{F}_t$ is of fixed dimemsion $k \times r$ and $\mathbb{E}\lVert\mathbf{F}_t\rVert^4\leq C $.
        \item For all $i \in \left[p\right]$, $j \in \left[q\right]$ and $t \in \left[T\right]$, $\mathbb{E}\left[e_{t,ij}\right]=0$ and $\mathbb{E}|e_{t,ij}|^8 \leq C$.\footnote{$\left[n\right] \overset{\Delta}{=} \{1,\dots,n\}$}
        \item Factor and noise are uncorrelated, that is, $\mathbb{E}\left[e_{t,ij}f_{s,lh}\right]=0$ for any $t,s \in \left[T\right]$, $i \in \left[p\right]$, $j \in \left[q\right]$, $l \in \left[k\right]$, $h \in \left[r\right]$.
    \end{enumerate}
\end{assumption}

\begin{assumption}\label{assumption 3 1.7.1}
    Loading matrix. \normalfont For each row of $\mathbf{R}$, $\lVert\mathbf{R}_{i\cdot}\rVert = \mathcal{O}(1)$, and, as $p,q \rightarrow \infty$, we have $\lVert p^{-1}\mathbf{R}^T\mathbf{R}-\Omega_R\rVert \rightarrow 0$ for some $k \times k$ positive definite matrix $\Omega_R$. For each row of $\mathbf{C}$, $\lVert\mathbf{C}_{i\cdot}\rVert = \mathcal{O}(1)$,and, as $p,q \rightarrow \infty$, we have $\lVert p^{-1}\mathbf{C}^T\mathbf{C}-\Omega_R\rVert \rightarrow 0$ for some $r \times r$ positive definite matrix $\Omega_C$.(\textbf{\textit{an extension of the pervasive assumption}}\footnote{Stock and Watson 2002})
\end{assumption}

\begin{assumption}\label{assumption 4 1.7.1}
    Cross row(column) correlation of noise $\mathbf{E}_t$.\normalfont  There exists some positive constant $C<\infty$ such that,
    \begin{enumerate}
        \item Let $\mathbf{U}_E = \mathbb{E}\left[\dfrac{1}{qT}\sum_{t=1}^T\mathbf{E}_t\mathbf{E}_t^T\right]$ and $\mathbf{V}_E = \mathbb{E}\left[\dfrac{1}{qT}\sum_{t=1}^T\mathbf{E}_t^T\mathbf{E}_t\right]$, we assume $\lVert\mathbf{U}_E\rVert_1\leq C$ and $\lVert\mathbf{V}_E\rVert_1\leq C$.
        \item For all row $i \in \left[p\right]$ and $j \in \left[q\right]$ and $t \in \left[T\right]$, we assume $\sum_{l \in p, l \neq i}\sum_{h \in q, h \neq j}|\mathbb{E}\left[e_{t,ij}e_{t,lh}\right]|\leq C$.
        \item For any row $i,l \in \left[p\right]$, any time $t \in \left[T\right]$, and any column $j \in \left[q\right]$,
        $$\sum\limits_{m \in \left[p\right]}\sum\limits_{s \in \left[T\right]}\sum\limits_{h \in \left[q\right], h \neq j}|cov\left[e_{t,ij}e_{t,lj},e_{s,ih}e_{s,mh}\right]| \leq C$$
        Similar, for any column $j,h \in \left[q\right]$, any time $t \in \left[T\right]$, and any row $i \in \left[p\right]$,
        $$\sum\limits_{m \in \left[q\right]}\sum\limits_{s \in \left[T\right]}\sum\limits_{l \in \left[p\right], l \neq i}|cov\left[e_{t,ij}e_{t,ih},e_{s,lj}e_{s,lm}\right]| \leq C$$
    \end{enumerate}
    
    (\textbf{\textit{automatically hold when the errors $\mathbf{E}_t$ are i.i.d. over rows and columns for any t, C for weak correlation}})
\end{assumption}

\begin{assumption}\label{assumption 5 1.7.1}
     $\mathbf{E}_t$.\normalfont  There exists $m>2$, $1<a,b<\infty$, $\frac{1}{a}+\frac{1}{b} = 1$, such that, for some positives $C<\infty$,
    \begin{enumerate}
        \item For any $l \in \left[k\right]$, $i \in \left[p\right]$, and $t \in \left[T\right]$, $\mathbb{E}\left[|\dfrac{1}{\sqrt{q}}\sum_{j=1}^{q}e_{t,ij}|^{mb}\right] = \mathcal{O}(1)$, $\mathbb{E}\left[\lVert\dfrac{1}{\sqrt{q}}\sum_{j=1}^{q}\mathbf{C}_{j\cdot}e_{t,ij}\rVert^{mb}\right] = \mathcal{O}(1)$, and $\mathbb{E}\left[\lVert\mathbf{f}_{t,l\cdot}\rVert^{ma}\right] \leq C$
        \item For any $h \in \left[r\right]$, $j \in \left[q\right]$, and $t \in \left[T\right]$, $\mathbb{E}\left[|\dfrac{1}{\sqrt{p}}\sum_{i=1}^{p}e_{t,ij}|^{mb}\right] = \mathcal{O}(1)$, $\mathbb{E}\left[\lVert\dfrac{1}{\sqrt{p}}\sum_{i=1}^{p}\mathbf{R}_{i\cdot}e_{t,ij}\rVert^{mb}\right] = \mathcal{O}(1)$, and $\mathbb{E}\left[\lVert\mathbf{f}_{t,\cdot h}\rVert^{ma}\right] \leq C$
        \item For any and $t \in \left[T\right]$, $\mathbb{E}\left[|\dfrac{1}{\sqrt{pq}}\sum_{i=1}^{p}\sum_{j=1}^qe_{t,ij}|^{mb}\right] = \mathcal{O}(1)$, $\mathbb{E}\left[\lVert\dfrac{1}{\sqrt{pq}}\sum_{i=1}^{p}\sum_{j=1}^q\mathbf{R}_{i\cdot}\mathbf{C}_{j\cdot}^Te_{t,ij}\rVert^{mb}\right] = \mathcal{O}(1)$.
        
    \end{enumerate}
    
    (\textbf{\textit{satisfied by Gaussian noise $\mathbf{E}_t$ with i.i.d. columns and rows}})
\end{assumption}


 

\subsection{Simulation}

\subsection{Application}
\section{High-Dimensional GLM with Binary Outcomes}
\subsection{Overall Summary}
\section [Ultra-High Dimensional GFM]{Ultra-High Dimensional GFM\footnote{Generalized Factor Model}} 
\subsection{Overall Summary} 

\section{Matrix-variate Logistic Regression with Measurement Error}
\section{A Likelihood-Based Approach for Multivariate Categorical Response Regression in High Dimensions}
\section{A likelihood-Based Approach for Semiparametric Regression with Panel Count Data}


\section{Time Series Latent Gaussian Count}
\section{Time Series Factor Models(tensor)}   

\end{document}
